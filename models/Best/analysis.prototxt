

name: "LUngNoduleAnalysis_project_CNN-version"


#------------------------Train Data---------------------------#
layer{
	name: "data"
	type: "Data"
	top: "data"
	top: "label"
	include { phase: TRAIN }
	data_param {
		backend: LMDB
		source: "./lmdb/train_Cls_lmdb"
		batch_size: 16
	}
}
#-------------------------Test Data---------------------------#
layer{
	name: "data"
	type: "Data"
	top: "data"
	top: "label"
	include { phase: TEST }
	data_param {
		backend: LMDB
		source: "./lmdb/val_Cls_lmdb"
		batch_size: 16
	}
}

#----------------------Convoltuion1_1-------------------------#
#64x64
layer {
        name: "conv1_1"
        type: "Convolution"
        bottom: "data"
        top: "conv1_1"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        convolution_param{
                num_output: 64
                pad: 1
                kernel_size: 7
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu1_1"
        type: "ReLU"
        bottom: "conv1_1"
        top: "conv1_1"
}
layer {
        name: "bn1_1"
        type: "BatchNorm"
        bottom: "conv1_1"
        top: "conv1_1"
}
#----------------------Convoltuion1_2-------------------------#
#64x64
layer {
        name: "conv1_2"
        type: "Convolution"
        bottom: "conv1_1"
        top: "conv1_2"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        convolution_param{
                num_output: 64
                pad: 1
                kernel_size: 7
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu1_2"
        type: "ReLU"
        bottom: "conv1_2"
        top: "conv1_2"
}
layer {
        name: "bn1_2"
        type: "BatchNorm"
        bottom: "conv1_2"
        top: "conv1_2"
}
#----------------------Convoltuion1_3-------------------------#
#64x64
layer {
        name: "conv1_3"
        type: "Convolution"
        bottom: "conv1_2"
        top: "conv1_3"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        convolution_param{
                num_output: 64
                pad: 1
                kernel_size: 7
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu1_3"
        type: "ReLU"
        bottom: "conv1_3"
        top: "conv1_3"
}
layer {
        name: "bn1_3"
        type: "BatchNorm"
        bottom: "conv1_3"
        top: "conv1_3"
}

layer {
	name: "pool1"
	type: "Pooling"
	bottom: "conv1_3"
	top: "pool1"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
#----------------------Convoltuion2_1-------------------------#
#32x32
layer {
	name: "conv2_1"
	type: "Convolution"
	bottom: "pool1"
	top: "conv2_1"
	param { lr_mult: 1 decay_mult:1 } 
	param { lr_mult: 2 decay_mult:0 }
	convolution_param{
		num_output: 128
		pad: 1
		kernel_size: 5
		weight_filler { type: "gaussian" std: 0.01 }
		bias_filler { type: "constant" value: 0 }
	}
}
layer {
	name: "relu2_1"
	type: "ReLU"
	bottom: "conv2_1"
	top: "conv2_1"
}
layer {
	name: "bn2_1"
	type: "BatchNorm"
	bottom: "conv2_1"
	top: "conv2_1"
}
#----------------------Convoltuion2_2-------------------------#
#32x32
layer {
        name: "conv2_2"
        type: "Convolution"
        bottom: "conv2_1"
        top: "conv2_2"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        convolution_param{
                num_output: 128
                pad: 1
                kernel_size: 5
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu2_2"
        type: "ReLU"
        bottom: "conv2_2"
        top: "conv2_2"
}
layer {
        name: "bn2_2"
        type: "BatchNorm"
        bottom: "conv2_2"
        top: "conv2_2"
}
#----------------------Convoltuion2_3-------------------------#
#32x32
layer {
        name: "conv2_3"
        type: "Convolution"
        bottom: "conv2_2"
        top: "conv2_3"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        convolution_param{
                num_output: 128
                pad: 1
                kernel_size: 5
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu2_3"
        type: "ReLU"
        bottom: "conv2_3"
        top: "conv2_3"
}
layer {
        name: "bn2_3"
        type: "BatchNorm"
        bottom: "conv2_3"
        top: "conv2_3"
}
layer {
	name: "pool2"
	type: "Pooling"
	bottom: "conv2_3"
	top: "pool2"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
#----------------------Convoltuion3_1-------------------------#
#16x16
layer {
	name: "conv3_1"
	type: "Convolution"
	bottom: "pool2"
	top: "conv3_1"
	param { lr_mult: 1 decay_mult:1 } 
	param { lr_mult: 2 decay_mult:0 }
	convolution_param{
		num_output: 256
		pad: 1
		kernel_size: 3
		weight_filler { type: "gaussian" std: 0.01 }
		bias_filler { type: "constant" value: 0 }
	}
}
layer {
	name: "relu3_1"
	type: "ReLU"
	bottom: "conv3_1"
	top: "conv3_1"
}
layer {
        name: "bn3_1"
        type: "BatchNorm"
        bottom: "conv3_1"
        top: "conv3_1"
}
#----------------------Convoltuion3_2-------------------------#
#16x16
layer {
        name: "conv3_2"
        type: "Convolution"
        bottom: "conv3_1"
        top: "conv3_2"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        convolution_param{
                num_output: 256
                pad: 1
                kernel_size: 3
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu3_2"
        type: "ReLU"
        bottom: "conv3_2"
        top: "conv3_2"
}
layer {
        name: "bn3_2"
        type: "BatchNorm"
        bottom: "conv3_2"
        top: "conv3_2"
}
layer {
	name: "pool3"
	type: "Pooling"
	bottom: "conv3_2"
	top: "pool3"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
#----------------------Convoltuion4-------------------------#
#8x8
layer {
	name: "conv4"
	type: "Convolution"
	bottom: "pool3"
	top: "conv4"
	param { lr_mult: 1 decay_mult:1 } 
	param { lr_mult: 2 decay_mult:0 }
	convolution_param{
		num_output: 512
		pad: 1
		kernel_size: 1
		weight_filler { type: "gaussian" std: 0.01 }
		bias_filler { type: "constant" value: 0 }
	}
}
layer {
	name: "relu4"
	type: "ReLU"
	bottom: "conv4"
	top: "conv4"
}
layer {
        name: "bn4"
        type: "BatchNorm"
        bottom: "conv4"
        top: "conv4"
}
layer {
	name: "pool4"
	type: "Pooling"
	bottom: "conv4"
	top: "pool4"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}
#---------------------Fully_Connected1------------------------#
#4x4
layer {
	name: "fc1"
	type: "InnerProduct"
	bottom: "pool4"
	top: "fc1"
	param { lr_mult: 1 decay_mult:1 } 
	param { lr_mult: 2 decay_mult:0 }
	inner_product_param{
		num_output: 64
		weight_filler { type: "gaussian" std: 0.01 }
		bias_filler { type: "constant" value: 0 }
	}
}
layer {
	name: "relu_fc1"
	type: "ReLU"
	bottom: "fc1"
	top: "fc1"
}
layer {
	name: "bnfc1"
	type: "BatchNorm"
	bottom: "fc1"
	top: "fc1"
}
#layer {
#	name: "drop1"
#	type: "Dropout"
#	bottom: "fc1"
#	top: "fc1"
#	dropout_param {
#		dropout_ratio: 0.5
#	}
#}
#---------------------Fully_Connected2------------------------#
#1x1
layer {
	name: "fc2"
	type: "InnerProduct"
	bottom: "fc1"
	top: "fc2"
	param { lr_mult: 1 decay_mult:1 } 
	param { lr_mult: 2 decay_mult:0 }
	inner_product_param{
		num_output: 1024
		weight_filler { type: "gaussian" std: 0.01 }
		bias_filler { type: "constant" value: 0 }
	}
}
layer {
	name: "relu_fc2"
	type: "ReLU"
	bottom: "fc2"
	top: "fc2"
}
layer {
	name: "bnfc2"
	type: "BatchNorm"
	bottom: "fc2"
	top: "fc2"
}
#layer {
#	name: "drop2"
#	type: "Dropout"
#	bottom: "fc2"
#	top: "fc2"
#	dropout_param {
#		dropout_ratio: 0.5
#	}
#}
#---------------------Fully_Connected3------------------------#
#1x1
layer {
        name: "fc3"
        type: "InnerProduct"
        bottom: "fc2"
        top: "fc3"
        param { lr_mult: 1 decay_mult:1 }
        param { lr_mult: 2 decay_mult:0 }
        inner_product_param{
                num_output: 3
                weight_filler { type: "gaussian" std: 0.01 }
                bias_filler { type: "constant" value: 0 }
        }
}
layer {
        name: "relu_fc3"
        type: "ReLU"
        bottom: "fc3"
        top: "fc3"
}
layer {
        name: "bnfc3"
        type: "BatchNorm"
        bottom: "fc3"
        top: "fc3"
}
#layer {
#       name: "drop2"
#       type: "Dropout"
#       bottom: "fc2"
#       top: "fc2"
#       dropout_param {
#               dropout_ratio: 0.5
#       }
#}


#--------------------------Result-----------------------------#
layer {
	name: "loss"
	type: "SoftmaxWithLoss"
	bottom: "fc3"
	bottom: "label"
	top: "loss"
}
layer {
	name: "accuracy"
	type: "Accuracy"
	bottom: "fc3"
	bottom: "label"
	top: "accuracy"
}
